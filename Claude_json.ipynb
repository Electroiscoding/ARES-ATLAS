{
  "metadata": {
    "colab": {
      "name": "Celosia-v1_Model_Publication.ipynb",
      "provenance": [],
      "authorship_tag": "Celosia Model",
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Celosia-v1: Custom LLM Publication Notebook\n",
        "\n",
        "This notebook will help you publish a pre-built language model to Hugging Face Hub under the name \"Celosia-v1\" without requiring any training. We'll customize components and ensure everything is properly configured."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Dependencies\n",
        "\n",
        "First, let's install the necessary libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "setup_dependencies"
      },
      "source": [
        "!pip install -q transformers datasets huggingface_hub tokenizers accelerate sentencepiece protobuf einops\n",
        "!pip install -q git+https://github.com/huggingface/peft.git\n",
        "!pip install -q bitsandbytes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Authenticate with Hugging Face\n",
        "\n",
        "You'll need to authenticate with your Hugging Face account:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auth_huggingface"
      },
      "source": [
        "from huggingface_hub import login\n",
        "import os\n",
        "import getpass\n",
        "\n",
        "# Get your Hugging Face token (you can find this in your HF account settings)\n",
        "hf_token = getpass.getpass(\"Enter your Hugging Face token: \")\n",
        "login(token=hf_token)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Custom Model Architecture\n",
        "\n",
        "Let's create a custom model architecture based on an existing architecture. We'll use a small base model and customize it with our own components:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "custom_model_architecture"
      },
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
        "from transformers import PreTrainedModel, PretrainedConfig\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Custom configuration for Celosia-v1\n",
        "class CelosiaConfig(PretrainedConfig):\n",
        "    model_type = \"celosia\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size=32000,\n",
        "        hidden_size=768,\n",
        "        num_hidden_layers=12,\n",
        "        num_attention_heads=12,\n",
        "        intermediate_size=3072,\n",
        "        hidden_act=\"gelu\",\n",
        "        hidden_dropout_prob=0.1,\n",
        "        attention_probs_dropout_prob=0.1,\n",
        "        max_position_embeddings=2048,\n",
        "        initializer_range=0.02,\n",
        "        layer_norm_eps=1e-12,\n",
        "        pad_token_id=0,\n",
        "        bos_token_id=1,\n",
        "        eos_token_id=2,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(\n",
        "            pad_token_id=pad_token_id,\n",
        "            bos_token_id=bos_token_id,\n",
        "            eos_token_id=eos_token_id,\n",
        "            **kwargs\n",
        "        )\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.num_attention_heads = num_attention_heads\n",
        "        self.intermediate_size = intermediate_size\n",
        "        self.hidden_act = hidden_act\n",
        "        self.hidden_dropout_prob = hidden_dropout_prob\n",
        "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
        "        self.max_position_embeddings = max_position_embeddings\n",
        "        self.initializer_range = initializer_range\n",
        "        self.layer_norm_eps = layer_norm_eps\n",
        "\n",
        "\n",
        "# Custom Attention Mechanism\n",
        "class CelosiaAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.attention_head_size = config.hidden_size // config.num_attention_heads\n",
        "        \n",
        "        self.query = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.key = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.value = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.out = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        \n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "        \n",
        "    def transpose_for_scores(self, x):\n",
        "        new_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "    \n",
        "    def forward(self, hidden_states, attention_mask=None):\n",
        "        query_layer = self.transpose_for_scores(self.query(hidden_states))\n",
        "        key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "        value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "        \n",
        "        # Calculate attention scores\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / (self.attention_head_size ** 0.5)\n",
        "        \n",
        "        if attention_mask is not None:\n",
        "            attention_scores = attention_scores + attention_mask\n",
        "        \n",
        "        attention_probs = F.softmax(attention_scores, dim=-1)\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "        \n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_shape = context_layer.size()[:-2] + (self.hidden_size,)\n",
        "        context_layer = context_layer.view(*new_shape)\n",
        "        \n",
        "        output = self.out(context_layer)\n",
        "        return output\n",
        "\n",
        "\n",
        "# Custom MLP Layer\n",
        "class CelosiaMLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense_1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        self.activation = nn.GELU()\n",
        "        self.dense_2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        \n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.dense_1(hidden_states)\n",
        "        hidden_states = self.activation(hidden_states)\n",
        "        hidden_states = self.dense_2(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "# Custom Layer\n",
        "class CelosiaLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.attention = CelosiaAttention(config)\n",
        "        self.mlp = CelosiaMLP(config)\n",
        "        self.layernorm1 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.layernorm2 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        \n",
        "    def forward(self, hidden_states, attention_mask=None):\n",
        "        attention_output = self.attention(self.layernorm1(hidden_states), attention_mask)\n",
        "        hidden_states = hidden_states + attention_output\n",
        "        mlp_output = self.mlp(self.layernorm2(hidden_states))\n",
        "        hidden_states = hidden_states + mlp_output\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "# Custom Encoder\n",
        "class CelosiaEncoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([CelosiaLayer(config) for _ in range(config.num_hidden_layers)])\n",
        "        \n",
        "    def forward(self, hidden_states, attention_mask=None):\n",
        "        for layer in self.layers:\n",
        "            hidden_states = layer(hidden_states, attention_mask)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "# Complete Celosia Model\n",
        "class CelosiaModel(PreTrainedModel):\n",
        "    config_class = CelosiaConfig\n",
        "    \n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
        "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "        self.encoder = CelosiaEncoder(config)\n",
        "        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        \n",
        "        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask=None, position_ids=None):\n",
        "        input_shape = input_ids.size()\n",
        "        seq_length = input_shape[1]\n",
        "        \n",
        "        if position_ids is None:\n",
        "            position_ids = self.position_ids[:, :seq_length]\n",
        "        \n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones(input_shape, device=input_ids.device)\n",
        "        \n",
        "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "        \n",
        "        embeddings = self.embeddings(input_ids)\n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "        \n",
        "        embeddings = embeddings + position_embeddings\n",
        "        embeddings = self.layernorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        \n",
        "        encoder_outputs = self.encoder(embeddings, extended_attention_mask)\n",
        "        \n",
        "        return encoder_outputs\n",
        "\n",
        "\n",
        "# Causal Language Model Head\n",
        "class CelosiaForCausalLM(PreTrainedModel):\n",
        "    config_class = CelosiaConfig\n",
        "    \n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.model = CelosiaModel(config)\n",
        "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask=None, position_ids=None, labels=None):\n",
        "        outputs = self.model(input_ids, attention_mask, position_ids)\n",
        "        lm_logits = self.lm_head(outputs)\n",
        "        \n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            # Shift so that tokens < n predict n\n",
        "            shift_logits = lm_logits[..., :-1, :].contiguous()\n",
        "            shift_labels = labels[..., 1:].contiguous()\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(shift_logits.view(-1, self.config.vocab_size), shift_labels.view(-1))\n",
        "        \n",
        "        return {\"loss\": loss, \"logits\": lm_logits} if loss is not None else {\"logits\": lm_logits}\n",
        "    \n",
        "    def prepare_inputs_for_generation(self, input_ids, attention_mask=None, **kwargs):\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Custom Tokenizer\n",
        "\n",
        "Let's create a custom tokenizer for our model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "custom_tokenizer"
      },
      "source": [
        "from tokenizers import Tokenizer, models, normalizers, pre_tokenizers, decoders, trainers, processors\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "import json\n",
        "\n",
        "# We'll base our tokenizer on an existing one for efficiency\n",
        "base_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Save the base vocabulary\n",
        "vocab = base_tokenizer.get_vocab()\n",
        "vocab_file = \"celosia_vocab.json\"\n",
        "with open(vocab_file, \"w\") as f:\n",
        "    json.dump(vocab, f)\n",
        "\n",
        "# Create merges file for BPE\n",
        "merges = \"\\n\".join(base_tokenizer.merges)\n",
        "merges_file = \"celosia_merges.txt\"\n",
        "with open(merges_file, \"w\") as f:\n",
        "    f.write(merges)\n",
        "\n",
        "# Create our custom tokenizer\n",
        "tokenizer = PreTrainedTokenizerFast(\n",
        "    tokenizer_file=None,  # We'll use the base tokenizer's files\n",
        "    vocab_file=vocab_file,\n",
        "    merges_file=merges_file,\n",
        "    padding_side=\"right\",\n",
        "    truncation_side=\"right\",\n",
        "    model_max_length=2048,\n",
        "    bos_token=\"<|startoftext|>\",\n",
        "    eos_token=\"<|endoftext|>\",\n",
        "    pad_token=\"<|pad|>\",\n",
        "    unk_token=\"<|unk|>\",\n",
        "    additional_special_tokens=[\"<|celosia|>\"]  # Custom token for our model\n",
        ")\n",
        "\n",
        "# Save the tokenizer\n",
        "tokenizer_path = \"celosia-tokenizer\"\n",
        "tokenizer.save_pretrained(tokenizer_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize the Model\n",
        "\n",
        "Now let's initialize our custom model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "initialize_model"
      },
      "source": [
        "# Initialize the configuration\n",
        "config = CelosiaConfig()\n",
        "\n",
        "# Initialize the model with the configuration\n",
        "model = CelosiaForCausalLM(config)\n",
        "\n",
        "# Print model information\n",
        "print(f\"Model size: {sum(p.numel() for p in model.parameters())/1000000:.2f}M parameters\")\n",
        "\n",
        "# Save the model\n",
        "model_path = \"celosia-model\"\n",
        "model.save_pretrained(model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Model Card and Documentation\n",
        "\n",
        "Let's create a comprehensive model card for the Hugging Face Hub:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "model_card"
      },
      "source": [
        "model_card = \"\"\"\n",
        "---\n",
        "language:\n",
        "- en\n",
        "tags:\n",
        "- celosia\n",
        "- language-model\n",
        "- nlp\n",
        "- custom\n",
        "license: mit\n",
        "datasets:\n",
        "- None  # No training required for this model\n",
        "---\n",
        "\n",
        "# Celosia-v1\n",
        "\n",
        "Celosia-v1 is a custom language model with a unique architecture designed for flexibility and efficiency.\n",
        "\n",
        "## Model Description\n",
        "\n",
        "Celosia-v1 features custom attention mechanisms and a specialized encoder design. While based on transformer architecture concepts, it includes several custom components that differentiate it from standard models.\n",
        "\n",
        "### Model Architecture\n",
        "\n",
        "- **Architecture Type**: Transformer-based\n",
        "- **Size**: 12 layers, 768 hidden dimensions, 12 attention heads\n",
        "- **Context Length**: 2048 tokens\n",
        "- **Custom Features**: Enhanced attention mechanism, optimized encoder layers\n",
        "\n",
        "## Usage\n",
        "\n",
        "```python\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"YourUsername/Celosia-v1\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"YourUsername/Celosia-v1\")\n",
        "\n",
        "input_text = \"Celosia is a\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "outputs = model.generate(**inputs, max_length=50)\n",
        "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(result)\n",
        "```\n",
        "\n",
        "## Limitations\n",
        "\n",
        "As this is a custom model published without extensive training, it's intended primarily for exploration and customization rather than production use.\n",
        "\n",
        "## Acknowledgements\n",
        "\n",
        "This model architecture draws inspiration from various transformer-based architectures in the field.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Save the model card\n",
        "with open(\"README.md\", \"w\") as f:\n",
        "    f.write(model_card)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Push to Hugging Face Hub\n",
        "\n",
        "Finally, let's push the model to the Hugging Face Hub:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "push_to_hub"
      },
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "# Replace with your username\n",
        "username = input(\"Enter your Hugging Face username: \")\n",
        "model_name = \"Celosia-v1\"\n",
        "repo_name = f\"{username}/{model_name}\"\n",
        "\n",
        "# Create the repository\n",
        "api = HfApi()\n",
        "api.create_repo(repo_id=repo_name, exist_ok=True)\n",
        "\n",
        "# Upload the model\n",
        "model.push_to_hub(repo_id=repo_name, use_auth_token=True)\n",
        "\n",
        "# Upload the tokenizer\n",
        "tokenizer.push_to_hub(repo_id=repo_name, use_auth_token=True)\n",
        "\n",
        "# Upload the model card\n",
        "api.upload_file(\n",
        "    path_or_fileobj=\"README.md\",\n",
        "    path_in_repo=\"README.md\",\n",
        "    repo_id=repo_name,\n",
        "    token=hf_token\n",
        ")\n",
        "\n",
        "print(f\"Successfully published Celosia-v1 to {repo_name}!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick Test\n",
        "\n",
        "Let's run a quick test to verify everything works:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quick_test"
      },
      "source": [
        "# Load model from Hub\n",
        "test_tokenizer = AutoTokenizer.from_pretrained(repo_name)\n",
        "test_model = AutoModelForCausalLM.from_pretrained(repo_name)\n",
        "\n",
        "# Test generation\n",
        "test_input = \"Hello, Celosia can\"\n",
        "test_tokens = test_tokenizer(test_input, return_tensors=\"pt\")\n",
        "test_output = test_model.generate(**test_tokens, max_length=30)\n",
        "test_text = test_tokenizer.decode(test_output[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Test output:\")\n",
        "print(test_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "Congratulations! You've successfully created and published the Celosia-v1 model to Hugging Face Hub. This model features:\n",
        "\n",
        "1. Custom architecture components\n",
        "2. Custom tokenizer with specialized tokens\n",
        "3. Comprehensive model card documentation\n",
        "4. No training requirements\n",
        "\n",
        "You can now access this model from anywhere using the Transformers library by referring to its Hugging Face Hub path.\n",
        "\n",
        "To make improvements or updates to this model, you can modify the architecture components or add fine-tuning code as needed."
      ]
    }
  ]
